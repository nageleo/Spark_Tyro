{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Recipes: Chapter 1: Introduction\n",
    "\n",
    "\n",
    "\n",
    "Hi, Welcome to training series on PySpark. The notebook series provide recipes that you can use out of the box for your Spark implemtation using Python API for Spark - PySpark. We would continue to update the notebook adding more recipes, and encourage others to share their implementation or solution to a business problem.\n",
    "\n",
    "In this notebook, we introduce you to Spark Concepts and importing data to get started with. Why are we covering importing data here, because it would be used to understand \"partitioning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in these cells is runable. \n",
    "# Click on this cell, then press Shift+Enter to run it, \n",
    "# or click the Run button in the toolbar.\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext <b>was</b> the main entry point for Spark functionality ad represents the connection to a Spark cluster. It is used to create RDDs, accumulators and broadcast variables on that cluster. \n",
    " \n",
    "<span style=\"color:green;\"><i>Extra Gyaan: An \"entry point\" is defined as a point where control is transferred from operating system to the provided program.</i></span>\n",
    "\n",
    "You can have only one SparkContext active per JVM. If you want to create a new one, you have to use the stop() method of the active SparkContext. This behavior can change though when we set the <i>spark.driver.allowMultipleContexts</i> configuration flag to true. However multiple SparkContext is not considered as a good practice. It would be hard to debug multiple active contexts, as the workflow are not \"completely isolated\", and potential failure of one context can impact another and could break the whole JVM. \n",
    "\n",
    "It is in consideration to have multiple SparkContext elegantly, and might be a part of future Spark releases.\n",
    "\n",
    "As of now REMEMBER \n",
    "- <b>Only one active SparkContext per JVM</b>.\n",
    "- <b>Not required, if you are using Spark 2.0 and above as the entry point</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completely optional to execute these lines of code - SparkContext would have \n",
    "# been anyways created via SparkSession\n",
    "\n",
    "sparkContext = pyspark.SparkContext(appName=\"Pyspark Recipes\")\n",
    "print(sparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback with SparkContext was its specific character regarding the processing context. Previously we had streamingContext for Streaming Data, sqlContext for SQL, hiveContext for Hive. \n",
    "\n",
    "If you are using Spark 2.0 or later, RDD along with DataSet and its subset DataFrames APIs are becoming the standard APIs and basic unit of data abstraction in Spark. So SparkSession was intrdocued for handling the new APIs. SparkSession, thus evolved to a role of a common entry point for all different pipelines. The instance of SparkSession is constructed with a builder common for all processing types - streaming, sql except Hive, which requires a call to enableHive() method. \n",
    "\n",
    "While we have seen that only one Spark Context can be there per JVM, one can have multiple SparkSessions. This is made possible because SparkSession acts as a wrapper for SparkContext, which is created implicitly by the builder without any extra configuration options. \n",
    "\n",
    "A word about Spark Configuration\n",
    "While we have .config(\"<configuration>\",\"<value>\") for SparkSession, we can set the configuration directly via SparkContext.\n",
    "    \n",
    "    \n",
    "While using traditional RDD objects approach, the onus of selecting the optimum optimization strategy was left on the developer, in DataFrame Spark abstracts the low-level methods, abstracting them, and deciding best optimization strategy for the developer.\n",
    "\n",
    "It is recommended that we use DataFrame in our Spark applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local\") \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "# The local here means - run spark locally with one worker thread. \n",
    "# For more details - We have touched the concept of having worker threads in \"Optimizing Spark\"\n",
    "\n",
    "# to get the configuration value you have to go via the sparkContext object provided by SparkSession\n",
    "sparkSession.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - # To see the complete list of configuration options available visit - https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "On execution of the above code, you would get various configuration options. The sparkSession.sparkContext.getConf().getAll(), would only show a limited set of output, and include those configurations that have been specified, like \"spark.driver.cores\" and \"spark.executor.memory\". If you had not set the \"spark.driver.cores\" and \"spark.executor.memory\" (even though we have use default values), they would not be shown in the \"sparkSession.sparkContext.getConf().getAll()\" command. Try commenting the lines beginning with \".config\" and check for yourselves.\n",
    "\n",
    "Spark properties mainly can be classified into two types: \n",
    "- one is related to deploy, like “spark.driver.memory”, “spark.executor.instances”, this kind of properties may not be affected when setting programmatically through SparkConf in runtime, or the behavior is depending on which cluster manager and deploy mode you choose, so it would be suggested to set through configuration file or spark-submit command line options; \n",
    "- another is mainly related to Spark runtime control, like “spark.task.maxFailures”, this kind of properties can be set in either way.\n",
    "\n",
    "Do visit the link for the list of configuration provided by Spark, to ensure that you only use appropriate configuration values while specifying it via SparkSession object. While we have discussed how to optimally use configuration in <b>\"Optimizing Spark\"</b>. We would also include some of the most important properties that is platform (local, YARN, Mesos, Kubernetes) agonostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;\"><i>Extra Gyaan</i></span>\n",
    "\n",
    "While introducing SparkSession, we have said that you have multiple SparkSession.\n",
    "\n",
    "Does that means each \"SparkSession configuration\" would be different...?\n",
    "<b>NO</b>, Remember, the configurations are set at the SparkContext level, and even though you have not used it as entry point, using SparkSession object instead - the configuration have to be same for all the SparkSessions objects. \n",
    "\n",
    "The factory method - getOrCreate() provided by Apache Spark, exactly does the role of preventing creation of multiple SparkContexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating another SparkSession object\n",
    "\n",
    "sparkSessionForWhateverPurpose = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local\") \\\n",
    "                .appName(\"This would override your old AppName\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "sparkSession == sparkSessionForWhateverPurpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;\"><i>Extra Gyaan (continued)</i></span>\n",
    "\n",
    "Hold it, when we said that we can have multiple SparkSession, how come the two different objects sparkSession and sparkSessionForWhateverPurpose be the same?\n",
    "\n",
    "Turns out, it is the property of the \"builder\", and if we use it we would get back the \"same\" SparkSession instance. \n",
    "\n",
    "So how do I create a new SparkSession?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSessionTake2 = sparkSession.newSession()\n",
    "sparkSession == sparkSessionTake2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;\"><i>More Extra Gyaan (continued)</i></span>\n",
    "\n",
    "Why would we require multiple SparkSessions?\n",
    "In case, when data is coming from two different sources. Say you want to compare two different table source  and destination from two separate hive servers.\n",
    "\n",
    "However, such use cases are outliers, and you might never ever want to have multiple SparkSessions in your project. :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Configuration in Spark\n",
    "\n",
    "There is a configuration property which shows the default minimum partitions based on starting the SparkContext. Remember we have specified the SparkContext via SparkSession with some default parameters, and specifying local cluster without any parameters, which means just one worker thread.\n",
    "\n",
    "<b>Remember</b> \n",
    "- local : Run Spark locally with one worker thread (i.e. no parallelism at all).\n",
    "- <b>local[*] Run Spark locally with as many worker threads as logical cores on your machine.</b>\n",
    "- local[n] : Run Spark locally with \"n\" worker threads (ideally, set this to the number of cores on your machine).\n",
    "- local[n,f] : Run Spark locally with \"n\" worker threads and \"f\" maxFailures (see spark.task.maxFailures for an explanation of this variable)\n",
    "- local[*,f] : Run Spark locally with as many worker threads as logical cores on your machine and \"f\" maxFailures.\n",
    "\n",
    "By default SparkSessions would set defaultMinPartitions as one Partition per core (worker thread). So let us execute the code and check the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at how partitioning work with spark. For more details on Partitioning refer to the document - optimizing Spark.\n",
    "\n",
    "The reason why we are discussing paritioning here is to demonstrate that while you can control partitioning using configuration option, it does not work in same way when you read from text files. So lets dive into Partitioning (did we tell you there would be a separate notebook on Partitioning :), ok now we are telling you - there is). \n",
    "\n",
    "As a first step, let us create some data. You might ask - why not read? Patience, we would be there in a little while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create some data first, don't worry about schema, we have it covered it in detail\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "start_date = date(2019, 1, 1)\n",
    "data = []\n",
    "for i in range(0, 50):\n",
    "    data.append({\"Country\": \"CN\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "    data.append({\"Country\": \"AU\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "\n",
    "schema = StructType([StructField('Country', StringType(), nullable=False),\n",
    "                     StructField('Date', DateType(), nullable=False),\n",
    "                     StructField('Amount', IntegerType(), nullable=False)])\n",
    "\n",
    "#this would display the data output - over 50 records\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using our default configuration of \"local\", let us create a dataframe and see how many partitions it would create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.createDataFrame(data, schema=schema)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It created \"1\" partition only. What if you change the above range to 5 million and try. Would Spark still create \"1\" partition or more? Try that, be patient, and let us know the results. \n",
    "\n",
    "Now we are going to recreate the sparkSession with configuration values, and see if it changes the partition. We are going to use \"local[4]\", which mean use four cores, to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[4] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us check, what impact does it has on the configuration parameter - \"defaultMinPartitions\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It shows the minimum Partition to be \"2\". The number of minimum Partition is always going to be 2, and would be reduced to one if you are utilizing only one core. Else, irrespective of he number of cores, allowing one to create that many partitions, the minimum partition would always be 2. \n",
    "\n",
    "Now, previously we have seen that when we used the .createDataFrame() method, it had created just one partition. Let us see the impact of using 4 cores for number of partitions. Would we get 4 partitions, 2 partitions or 1 partition?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.createDataFrame(data, schema=schema)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default Spark would always create one partition per core. If you have 16 cores, you would have 16 partitions.\n",
    "\n",
    "Now, let us try to limit the number of driver cores using Spark Configuration. Remember our local[4] is still there, and we are going to specify the limit on spark.driver.cores configuration to 2. Would we now get 4 partitions as above, or 2 partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[2] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .config(\"spark.driver.cores\", '2') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "df = sparkSession.createDataFrame(data, schema=schema)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So changing the configuration \"spark.driver.cores\" did not help. We still have 4 partitions, where the number of cores specified in the local cluster overrule the configuration \"spark.driver.cores\". \n",
    "\n",
    "Let us look at other configuration which is \"spark.default.parallelism\". We would set it to 2. Would this limit the number of partitions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[2] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .config(\"spark.driver.cores\", '2') \\\n",
    "                .config(\"spark.default.parallelism\", '3') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "df = sparkSession.createDataFrame(data, schema=schema)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It used four partitions. So even though we had mentioned to use local[4], that is use 4 cores, but we limited the spark.default.parallelism to have 3 partitions via the configuration parameter \"spark.default.parallelism\"\n",
    "\n",
    "Let us see if we had specified just 2 worker threads and set the default parallelism to 3, would we get 2 partitions or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[2] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[2]\") \\\n",
    "                .config(\"spark.driver.cores\", '2') \\\n",
    "                .config(\"spark.default.parallelism\", '5') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "df = sparkSession.createDataFrame(data, schema=schema)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "As we can see from the above output, even when we declared 2 cores to be utilized, we still get \"5\" partitions as governed by the configuration value spark.default.parallelism, which we had set to 5. Try changing it to 3 or 5 and check the output. \n",
    "\n",
    "Now, would we expect the same behaviour when we are importing data by reading from CSV files, Text files?  Before that let us go back to the setting that gave 4 worker threads and 3 default parallelism. We expect that we would be getting 3 partitions when it is reading the data from the CSV file.\n",
    "\n",
    "Let us see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[2] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .config(\"spark.driver.cores\", '2') \\\n",
    "                .config(\"spark.default.parallelism\", '3') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "\n",
    "# Don't worry about this - we would be comfortable with these concept in next notebook\n",
    "dfCensus = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                    ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/censusdata.csv')\n",
    "\n",
    "dfCensus.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are writing a small function to get the length of partition\n",
    "def getParitions(dfTargetDataFrame):\n",
    "    l = dfTargetDataFrame.rdd.glom().map(len).collect()\n",
    "    print('Min Parition Size: ',min(l),'. Max Parition Size: ', max(l),'. Avg Parition Size: ',\n",
    "           sum(l)/len(l),'. Total Partitions: ', len(l))\n",
    "\n",
    "getParitions(dfCensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided you have executed it on local computer, you might see the output for the above lines as follows:<br/>\n",
    "<b>Min Parition Size:  9831 . Max Parition Size:  35391 . Avg Parition Size:  22611.0 . Total Partitions:  2</b>\n",
    "\n",
    "But in our previous attempt we were having three partitions by using 4 cores, and default parallelism as 3. If you even remove the config default parallelism, and use 4 cores, you still would not end up with one partition per core. Ideally one should get 3/4 partitions depending on the configurations, but in this case Spark is not utilizing the parititons it can create?\n",
    "\n",
    "It turns out that Spark while importing data - reads the data in blocks of 128 MB (the default Hadoop block size), and it would have either \"total file size\"/128 partitions or 2 (the default mininum partitions, whichever is more. Since our data is less than 128 MB (around 6 MB), we would be having one block, but the default minimum partition is 2, and hence, it would decide to use 2 partitions.\n",
    "\n",
    "This is the default partitioning of Spark. If you want to increase the number of partitions, you need to execute the following command-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCensus= dfCensus.repartition(10)\n",
    "getParitions(dfCensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you ask Spark to partition the data in 10 different partitions, it would take the total of 45,222 records and create a partition of 4522 each,and you would see an output for the above lines as follows:<br/>\n",
    "<b>Min Parition Size:  4522 . Max Parition Size:  4523 . Avg Parition Size:  4522.2 . Total Partitions:  10</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCensus.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final word about Spark Partitions, we have already seen how to use the .config in method chaining to specifiy the configurations. There are alternate ways of setting the configurations which we have demonstrated here. Feel free to play around with the configurations, and check the result for yourselves - \"Learning by Doing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local\") \\\n",
    "                .config(\"spark.driver.cores\", '2') \\\n",
    "                .config(\"spark.executor.memory\", '1g') \\\n",
    "                .config(\"spark.default.parallelism\", '4') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "# One can also set Spark Configuration using following syntax-\n",
    "#spark_session.conf.set(\"spark.driver.cores\", '1')\n",
    "#spark_session.conf.set(\"spark.executor.memory\", '1g')\n",
    "\n",
    "# to get the configuration value you have to go via the sparkContext object provided by SparkSession\n",
    "sparkSession.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, we would do one more exercise. We would specify the cores to be utilized as 4 and the configuration parameter - \"spark.default.parallelism\" as \"1\". \n",
    "\n",
    "Then we would check for configuration property - defaultMinPartitions (remember, even with more than one \"spark.default.parallelism\" we had defaultMinPartitions as always two. We are trying to see if it becomes one, which is possible - as we have seen when we had utilized only one core)\n",
    "\n",
    "We would also read the CSV file, and check for the number of partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the existing spark session\n",
    "sparkSession.stop()\n",
    "\n",
    "#The only change we did in the above initialization of SparkSession is to use local[2] which means \n",
    "# we are calling \n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .config(\"spark.default.parallelism\", '1') \\\n",
    "                .appName(\"Pyspark Recipes\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "#Checking for defaultMinPartitions\n",
    "sparkSession.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this - we would be comfortable with these concept in next notebook\n",
    "dfCensus = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                    ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/censusdata.csv')\n",
    "\n",
    "getParitions(dfCensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With this we conclude our introduction to Spark, and in the next section, we would look at importing data. So let us move to our next notebook. \n",
    "\n",
    "Don't forget to close your SparkSesssion when you are done - as it is a good practice to free cluster resources for other applications. SparkSession \"stop()\" eventually calls \"SparkContext stop()\". However be mindful, you might have multiple SparkSession, and in the end you have to ensure that all SparkSessions have finished their job. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
