{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Recipes: Chapter 4: Joins\n",
    "\n",
    "\n",
    "We would be working with a sample dataset unrelated to our census data for our exercise, so that you appreciate different kind of joins made possible by Spark. \n",
    "\n",
    "We would start with regular joins - Inner, Left, Right and Outer joins and then talk about Anti, Semi,Cross and Full Joins as we progress. \n",
    "\n",
    "Summary of supported join types - \n",
    "- Inner Join ('inner') \n",
    "- Left Join ('left', 'leftouter', 'left_outer')\n",
    "- Right Join ('right', 'rightouter', 'right_outer')\n",
    "- Outer/Full Joins ('outer', 'full', 'fullouter', 'full_outer')\n",
    "- Left Semi Join ('leftsemi', 'left_semi')\n",
    "- Left Anti Semi Join ('leftanti', 'left_anti')\n",
    "- Cross Join ('cross')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# creating a SparkSession object - you can change any of the configuration option you like. Remember this would\n",
    "# get the existsing SparkSession and would not create a new one.\n",
    "# So in case your previous notebook is still running - no issues.\n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local\") \\\n",
    "                .appName(\"Pyspark Recipes - Joins\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "\n",
    "# Name of the Participants and their final marks. Ignore Fractal_ID, :), \n",
    "# Scores are purely random, so if you got score not as per your expectation, blame it on chance. \n",
    "dfScores = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/dataScores.csv')\n",
    "\n",
    "# Location of the Participants - some of the participants location have been delete\n",
    "# some dummy Ids and location have been added.\n",
    "dfLocation = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/dataLocation.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+\n",
      "|Fractal_ID|               Name|Scores|\n",
      "+----------+-------------------+------+\n",
      "|         1|     Piyusha Biswas|    99|\n",
      "|         1|     Piyusha Biswas|    89|\n",
      "|         2|Siddhartha Nuthakki|    92|\n",
      "|         3|     Phani Kompella|    86|\n",
      "|         4|   Gaurav Acharekar|    93|\n",
      "|         5|       Shadab Azeem|    96|\n",
      "|         6|       Rachit Sapra|    86|\n",
      "|         7|      Tulika Mittal|    90|\n",
      "|         8|          Narhari B|    81|\n",
      "|         9|       Akash Saxena|    94|\n",
      "|        10|        Heba Nomani|    96|\n",
      "|        11|      Manish Shukla|    96|\n",
      "|        12|   Aishwary Mandloi|    84|\n",
      "|        13|      Praveen Nagel|    91|\n",
      "|        14|         Viral Jani|    90|\n",
      "|        15|      Charls Joseph|    81|\n",
      "|        16|     Affan Mohammed|    88|\n",
      "|        17|       Ashish Aswal|   100|\n",
      "|        18|         Santosh Tv|    88|\n",
      "|        19|        Maaz Ansari|    98|\n",
      "|        20|   Sangamesh Kalagi|    82|\n",
      "|        21|       Madhusudan B|    94|\n",
      "|        22|        Muniswamy S|    82|\n",
      "|        23|     Prashant Yadav|    99|\n",
      "|        24|          Surya Das|    90|\n",
      "|        25| Venkata Chippagiri|    96|\n",
      "|        26|       Versha Singh|    93|\n",
      "|        27|  Mayank Srivastava|    87|\n",
      "|        28|      Mithu Goswami|    99|\n",
      "|        29|     Sandeep Challa|    83|\n",
      "+----------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe that dfScores have a total of 30 rows,\n",
    "# and Piyusha Biswas has two scores (nothing intentional, the name figured first in the email list)\n",
    "dfScores.show(n=30) \n",
    "dfScores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|Fractal_ID|       Country|          City|\n",
      "+----------+--------------+--------------+\n",
      "|         1| United States|    Louisville|\n",
      "|         1|         India|       Gurgaon|\n",
      "|         3|         India|       Gurgaon|\n",
      "|         4|         India|        Mumbai|\n",
      "|         5|         India|       Gurgaon|\n",
      "|         6|         India|       Gurgaon|\n",
      "|         7|United Kingdom|        London|\n",
      "|         8|         India|     Bengaluru|\n",
      "|         9|         India|       Gurgaon|\n",
      "|        10|         India|        Mumbai|\n",
      "|        11|         India|       Gurgaon|\n",
      "|        12|         India|     Bengaluru|\n",
      "|        13|         India|       Gurgaon|\n",
      "|        14|         India|        Mumbai|\n",
      "|        15| United States|       Atlanta|\n",
      "|        18|         India|     Bengaluru|\n",
      "|        19|         India|        Mumbai|\n",
      "|        20|         India|     Bengaluru|\n",
      "|        24|         India|        Mumbai|\n",
      "|        25| United States|      New York|\n",
      "|        27| United States|North Carolina|\n",
      "|        28|         India|        Mumbai|\n",
      "|        29|         India|     Bengaluru|\n",
      "|        30|         India|     Bengaluru|\n",
      "|        31|         India|        Mumbai|\n",
      "|        32|         India|       Gurgaon|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe that dfLocations have 26 rows, \n",
    "# data for Fractal_ID - 2,16,17,21,22,23,26 are not there.\n",
    "# Our target Biswasji, has two entries.\n",
    "# and records with Fractal_ID - 30,31,32 do not exists in dfScores\n",
    "dfLocation.show(n=26) \n",
    "dfLocation.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner joins\n",
    "\n",
    "Also known as \"natural joins\", it returns the rows when the matching conditions are met. This is also the default join that Spark would use, if no join type is provided - i.e. <span style=\"color:blue\">how='join type'</span>, and a \"join condition\", i.e. \"on\" is provided. \n",
    "\n",
    "If you observe the output - you can see of course missing Fractal IDs - 2,16,17,21,22,23,26 are not there. Another interesting observation is Piyusha Biswas is repeated \"4\" times. If the dfLocation just had one entry for the Fractal_ID - \"1\", we would have seen Piyusha Biswas being repeated twice, with two different country and cities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+--------------+--------------+\n",
      "|Fractal_ID|              Name|Scores|       Country|          City|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "|         1|    Piyusha Biswas|    99|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    99| United States|    Louisville|\n",
      "|         1|    Piyusha Biswas|    89|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    89| United States|    Louisville|\n",
      "|         3|    Phani Kompella|    86|         India|       Gurgaon|\n",
      "|         4|  Gaurav Acharekar|    93|         India|        Mumbai|\n",
      "|         5|      Shadab Azeem|    96|         India|       Gurgaon|\n",
      "|         6|      Rachit Sapra|    86|         India|       Gurgaon|\n",
      "|         7|     Tulika Mittal|    90|United Kingdom|        London|\n",
      "|         8|         Narhari B|    81|         India|     Bengaluru|\n",
      "|         9|      Akash Saxena|    94|         India|       Gurgaon|\n",
      "|        10|       Heba Nomani|    96|         India|        Mumbai|\n",
      "|        11|     Manish Shukla|    96|         India|       Gurgaon|\n",
      "|        12|  Aishwary Mandloi|    84|         India|     Bengaluru|\n",
      "|        13|     Praveen Nagel|    91|         India|       Gurgaon|\n",
      "|        14|        Viral Jani|    90|         India|        Mumbai|\n",
      "|        15|     Charls Joseph|    81| United States|       Atlanta|\n",
      "|        18|        Santosh Tv|    88|         India|     Bengaluru|\n",
      "|        19|       Maaz Ansari|    98|         India|        Mumbai|\n",
      "|        20|  Sangamesh Kalagi|    82|         India|     Bengaluru|\n",
      "|        24|         Surya Das|    90|         India|        Mumbai|\n",
      "|        25|Venkata Chippagiri|    96| United States|      New York|\n",
      "|        27| Mayank Srivastava|    87| United States|North Carolina|\n",
      "|        28|     Mithu Goswami|    99|         India|        Mumbai|\n",
      "|        29|    Sandeep Challa|    83|         India|     Bengaluru|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfInnerJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='inner')\n",
    "\n",
    "dfInnerJoin = dfScores.join(dfLocation, on=['Fractal_ID'])\n",
    "\n",
    "dfInnerJoin.show(n=25)\n",
    "dfInnerJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, let us take a look at the schema of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fractal_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Scores: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfInnerJoin.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While we stick to the above syntax style, the following is completely acceptable.One can completely ignore specifying the \"on\" and \"how\", however maintain the order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+--------------+--------------+\n",
      "|Fractal_ID|              Name|Scores|       Country|          City|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "|         1|    Piyusha Biswas|    99|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    99| United States|    Louisville|\n",
      "|         1|    Piyusha Biswas|    89|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    89| United States|    Louisville|\n",
      "|         3|    Phani Kompella|    86|         India|       Gurgaon|\n",
      "|         4|  Gaurav Acharekar|    93|         India|        Mumbai|\n",
      "|         5|      Shadab Azeem|    96|         India|       Gurgaon|\n",
      "|         6|      Rachit Sapra|    86|         India|       Gurgaon|\n",
      "|         7|     Tulika Mittal|    90|United Kingdom|        London|\n",
      "|         8|         Narhari B|    81|         India|     Bengaluru|\n",
      "|         9|      Akash Saxena|    94|         India|       Gurgaon|\n",
      "|        10|       Heba Nomani|    96|         India|        Mumbai|\n",
      "|        11|     Manish Shukla|    96|         India|       Gurgaon|\n",
      "|        12|  Aishwary Mandloi|    84|         India|     Bengaluru|\n",
      "|        13|     Praveen Nagel|    91|         India|       Gurgaon|\n",
      "|        14|        Viral Jani|    90|         India|        Mumbai|\n",
      "|        15|     Charls Joseph|    81| United States|       Atlanta|\n",
      "|        18|        Santosh Tv|    88|         India|     Bengaluru|\n",
      "|        19|       Maaz Ansari|    98|         India|        Mumbai|\n",
      "|        20|  Sangamesh Kalagi|    82|         India|     Bengaluru|\n",
      "|        24|         Surya Das|    90|         India|        Mumbai|\n",
      "|        25|Venkata Chippagiri|    96| United States|      New York|\n",
      "|        27| Mayank Srivastava|    87| United States|North Carolina|\n",
      "|        28|     Mithu Goswami|    99|         India|        Mumbai|\n",
      "|        29|    Sandeep Challa|    83|         India|     Bengaluru|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInnerJoin = dfScores.join(dfLocation, \"Fractal_ID\", 'inner')\n",
    "dfInnerJoin.show(n=25)\n",
    "dfInnerJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What if the column names on which join are going to be placed are different. One can specify both the columns from different dataframes.\n",
    "\n",
    "While the output is the same, we can see one \"difference\" here - the column \"Fractal_ID\" is repeated. In previous section, Spark knows that the \"Fractal_ID\" is same and it picks one, however when you explicitly use \"==\", it treats them as two different column, while evaluating the boolean expression, and return them as a part of your resulting dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+----------+--------------+--------------+\n",
      "|Fractal_ID|              Name|Scores|Fractal_ID|       Country|          City|\n",
      "+----------+------------------+------+----------+--------------+--------------+\n",
      "|         1|    Piyusha Biswas|    99|         1|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    99|         1| United States|    Louisville|\n",
      "|         1|    Piyusha Biswas|    89|         1|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    89|         1| United States|    Louisville|\n",
      "|         3|    Phani Kompella|    86|         3|         India|       Gurgaon|\n",
      "|         4|  Gaurav Acharekar|    93|         4|         India|        Mumbai|\n",
      "|         5|      Shadab Azeem|    96|         5|         India|       Gurgaon|\n",
      "|         6|      Rachit Sapra|    86|         6|         India|       Gurgaon|\n",
      "|         7|     Tulika Mittal|    90|         7|United Kingdom|        London|\n",
      "|         8|         Narhari B|    81|         8|         India|     Bengaluru|\n",
      "|         9|      Akash Saxena|    94|         9|         India|       Gurgaon|\n",
      "|        10|       Heba Nomani|    96|        10|         India|        Mumbai|\n",
      "|        11|     Manish Shukla|    96|        11|         India|       Gurgaon|\n",
      "|        12|  Aishwary Mandloi|    84|        12|         India|     Bengaluru|\n",
      "|        13|     Praveen Nagel|    91|        13|         India|       Gurgaon|\n",
      "|        14|        Viral Jani|    90|        14|         India|        Mumbai|\n",
      "|        15|     Charls Joseph|    81|        15| United States|       Atlanta|\n",
      "|        18|        Santosh Tv|    88|        18|         India|     Bengaluru|\n",
      "|        19|       Maaz Ansari|    98|        19|         India|        Mumbai|\n",
      "|        20|  Sangamesh Kalagi|    82|        20|         India|     Bengaluru|\n",
      "|        24|         Surya Das|    90|        24|         India|        Mumbai|\n",
      "|        25|Venkata Chippagiri|    96|        25| United States|      New York|\n",
      "|        27| Mayank Srivastava|    87|        27| United States|North Carolina|\n",
      "|        28|     Mithu Goswami|    99|        28|         India|        Mumbai|\n",
      "|        29|    Sandeep Challa|    83|        29|         India|     Bengaluru|\n",
      "+----------+------------------+------+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInnerJoin = dfScores.join(dfLocation, dfScores.Fractal_ID == dfLocation.Fractal_ID, 'inner')\n",
    "dfInnerJoin.show(n=25)\n",
    "dfInnerJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;\"><i>Extra Gyaan:</span>\n",
    "The \"on\" condition is evaluated as a boolean. When we specify the \"Fractal_ID\", as a parameter, Spark searches for \"Fractal_ID\", in both the dataframe and does a boolean \"==\" check on them. The joins are evaluated on the basis of the boolean value. The code below would produce an error  - it would crib, it does not have a second column name for the boolean condition to be satisfied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- This would produce an error -----\n",
    "dfInnerJoin = dfScores.join(dfLocation, dfScores.Fractal_ID, 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would continue to stick to the first line syntax in this code window- for remaining demos. However, feel free to use the syntax you are comfortable with\n",
    "\n",
    "## Left Join\n",
    "\n",
    "Left join would return all the rows from the left dataframe (in this case dfScores), and the matched rows from the right dataframe (dfLocation)\n",
    "\n",
    "If you observe the output - you can see of course missing Fractal IDs - 2,16,17,21,22,23,26 are there and their corresponding Country and City are replaced with null. \n",
    "\n",
    "Another interesting observation is Piyusha Biswas is repeated \"4\" times - Again !!!. If the dfLocation just had one entry for the Fractal_ID - \"1\", we would have seen Piyusha Biswas being repeated twice, with two different country and cities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+--------------+----------+\n",
      "|Fractal_ID|               Name|Scores|       Country|      City|\n",
      "+----------+-------------------+------+--------------+----------+\n",
      "|         1|     Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|     Piyusha Biswas|    99| United States|Louisville|\n",
      "|         1|     Piyusha Biswas|    89|         India|   Gurgaon|\n",
      "|         1|     Piyusha Biswas|    89| United States|Louisville|\n",
      "|         2|Siddhartha Nuthakki|    92|          null|      null|\n",
      "|         3|     Phani Kompella|    86|         India|   Gurgaon|\n",
      "|         4|   Gaurav Acharekar|    93|         India|    Mumbai|\n",
      "|         5|       Shadab Azeem|    96|         India|   Gurgaon|\n",
      "|         6|       Rachit Sapra|    86|         India|   Gurgaon|\n",
      "|         7|      Tulika Mittal|    90|United Kingdom|    London|\n",
      "|         8|          Narhari B|    81|         India| Bengaluru|\n",
      "|         9|       Akash Saxena|    94|         India|   Gurgaon|\n",
      "|        10|        Heba Nomani|    96|         India|    Mumbai|\n",
      "|        11|      Manish Shukla|    96|         India|   Gurgaon|\n",
      "|        12|   Aishwary Mandloi|    84|         India| Bengaluru|\n",
      "|        13|      Praveen Nagel|    91|         India|   Gurgaon|\n",
      "|        14|         Viral Jani|    90|         India|    Mumbai|\n",
      "|        15|      Charls Joseph|    81| United States|   Atlanta|\n",
      "|        16|     Affan Mohammed|    88|          null|      null|\n",
      "|        17|       Ashish Aswal|   100|          null|      null|\n",
      "+----------+-------------------+------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLeftJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='left')\n",
    "dfLeftJoin.show()\n",
    "dfLeftJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Join\n",
    "\n",
    "Right join is just opposite of Left Join, which would return all the rows from the right dataframe (dfLocation) and the matched rows from the left dataframe (dfScores)\n",
    "\n",
    "Observe that Fractal IDs - 30, 31 and 32 which were only there in dfLocation are returned here, and the Name and Scores are replaced with null. Biswasji, is again repeated four times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+--------------+--------------+\n",
      "|Fractal_ID|              Name|Scores|       Country|          City|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "|         1|    Piyusha Biswas|    89| United States|    Louisville|\n",
      "|         1|    Piyusha Biswas|    99| United States|    Louisville|\n",
      "|         1|    Piyusha Biswas|    89|         India|       Gurgaon|\n",
      "|         1|    Piyusha Biswas|    99|         India|       Gurgaon|\n",
      "|         3|    Phani Kompella|    86|         India|       Gurgaon|\n",
      "|         4|  Gaurav Acharekar|    93|         India|        Mumbai|\n",
      "|         5|      Shadab Azeem|    96|         India|       Gurgaon|\n",
      "|         6|      Rachit Sapra|    86|         India|       Gurgaon|\n",
      "|         7|     Tulika Mittal|    90|United Kingdom|        London|\n",
      "|         8|         Narhari B|    81|         India|     Bengaluru|\n",
      "|         9|      Akash Saxena|    94|         India|       Gurgaon|\n",
      "|        10|       Heba Nomani|    96|         India|        Mumbai|\n",
      "|        11|     Manish Shukla|    96|         India|       Gurgaon|\n",
      "|        12|  Aishwary Mandloi|    84|         India|     Bengaluru|\n",
      "|        13|     Praveen Nagel|    91|         India|       Gurgaon|\n",
      "|        14|        Viral Jani|    90|         India|        Mumbai|\n",
      "|        15|     Charls Joseph|    81| United States|       Atlanta|\n",
      "|        18|        Santosh Tv|    88|         India|     Bengaluru|\n",
      "|        19|       Maaz Ansari|    98|         India|        Mumbai|\n",
      "|        20|  Sangamesh Kalagi|    82|         India|     Bengaluru|\n",
      "|        24|         Surya Das|    90|         India|        Mumbai|\n",
      "|        25|Venkata Chippagiri|    96| United States|      New York|\n",
      "|        27| Mayank Srivastava|    87| United States|North Carolina|\n",
      "|        28|     Mithu Goswami|    99|         India|        Mumbai|\n",
      "|        29|    Sandeep Challa|    83|         India|     Bengaluru|\n",
      "|        30|              null|  null|         India|     Bengaluru|\n",
      "|        31|              null|  null|         India|        Mumbai|\n",
      "|        32|              null|  null|         India|       Gurgaon|\n",
      "+----------+------------------+------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRightJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='right')\n",
    "dfRightJoin.show(n=28)\n",
    "dfRightJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Join\n",
    "\n",
    "Outer Join combines the result of both left and right joins, i.e., all the records would be returned from both the tables. \n",
    "\n",
    "Observe the order in which the data is returned, unlike previous outputs the column used for join is not ordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+--------------+--------------+\n",
      "|Fractal_ID|               Name|Scores|       Country|          City|\n",
      "+----------+-------------------+------+--------------+--------------+\n",
      "|        31|               null|  null|         India|        Mumbai|\n",
      "|        28|      Mithu Goswami|    99|         India|        Mumbai|\n",
      "|        26|       Versha Singh|    93|          null|          null|\n",
      "|        27|  Mayank Srivastava|    87| United States|North Carolina|\n",
      "|        12|   Aishwary Mandloi|    84|         India|     Bengaluru|\n",
      "|        22|        Muniswamy S|    82|          null|          null|\n",
      "|         1|     Piyusha Biswas|    99| United States|    Louisville|\n",
      "|         1|     Piyusha Biswas|    99|         India|       Gurgaon|\n",
      "|         1|     Piyusha Biswas|    89| United States|    Louisville|\n",
      "|         1|     Piyusha Biswas|    89|         India|       Gurgaon|\n",
      "|        13|      Praveen Nagel|    91|         India|       Gurgaon|\n",
      "|         6|       Rachit Sapra|    86|         India|       Gurgaon|\n",
      "|        16|     Affan Mohammed|    88|          null|          null|\n",
      "|         3|     Phani Kompella|    86|         India|       Gurgaon|\n",
      "|        20|   Sangamesh Kalagi|    82|         India|     Bengaluru|\n",
      "|         5|       Shadab Azeem|    96|         India|       Gurgaon|\n",
      "|        19|        Maaz Ansari|    98|         India|        Mumbai|\n",
      "|        15|      Charls Joseph|    81| United States|       Atlanta|\n",
      "|         9|       Akash Saxena|    94|         India|       Gurgaon|\n",
      "|        17|       Ashish Aswal|   100|          null|          null|\n",
      "|         4|   Gaurav Acharekar|    93|         India|        Mumbai|\n",
      "|         8|          Narhari B|    81|         India|     Bengaluru|\n",
      "|        23|     Prashant Yadav|    99|          null|          null|\n",
      "|         7|      Tulika Mittal|    90|United Kingdom|        London|\n",
      "|        10|        Heba Nomani|    96|         India|        Mumbai|\n",
      "|        25| Venkata Chippagiri|    96| United States|      New York|\n",
      "|        24|          Surya Das|    90|         India|        Mumbai|\n",
      "|        29|     Sandeep Challa|    83|         India|     Bengaluru|\n",
      "|        21|       Madhusudan B|    94|          null|          null|\n",
      "|        32|               null|  null|         India|       Gurgaon|\n",
      "|        11|      Manish Shukla|    96|         India|       Gurgaon|\n",
      "|        14|         Viral Jani|    90|         India|        Mumbai|\n",
      "|         2|Siddhartha Nuthakki|    92|          null|          null|\n",
      "|        30|               null|  null|         India|     Bengaluru|\n",
      "|        18|         Santosh Tv|    88|         India|     Bengaluru|\n",
      "+----------+-------------------+------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOuterJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='outer')\n",
    "dfOuterJoin.show(n=35)\n",
    "dfOuterJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Anti Join\n",
    "\n",
    "Returns those rows from the first dataframe which do not have any matches in the second dataframe. You can visualize it as leftovers - (LeftDataFrame - RightDataFrame). \n",
    "\n",
    "You can use how='leftanti' or how='left_anti'. Both the 'leftanti' and 'left_anti' are same. There is no 'right anti'. \n",
    "\n",
    "Try switching the places of dfScores and dfLocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+\n",
      "|Fractal_ID|               Name|Scores|\n",
      "+----------+-------------------+------+\n",
      "|         2|Siddhartha Nuthakki|    92|\n",
      "|        16|     Affan Mohammed|    88|\n",
      "|        17|       Ashish Aswal|   100|\n",
      "|        21|       Madhusudan B|    94|\n",
      "|        22|        Muniswamy S|    82|\n",
      "|        23|     Prashant Yadav|    99|\n",
      "|        26|       Versha Singh|    93|\n",
      "+----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLeftAntiJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='leftanti')\n",
    "dfLeftAntiJoin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Semi Join\n",
    "\n",
    "They are like the inner joins, except only the left dataframe columns and values are returned. Nothing is returned from the right dataframe. \n",
    "\n",
    "You can use how='leftsemi' or how='left_semi'. Both the 'leftsemi' and 'left_semi' are same. There is no 'right semi'. \n",
    "\n",
    "Try switching the places of dfScores and dfLocation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+\n",
      "|Fractal_ID|              Name|Scores|\n",
      "+----------+------------------+------+\n",
      "|         1|    Piyusha Biswas|    99|\n",
      "|         1|    Piyusha Biswas|    89|\n",
      "|         3|    Phani Kompella|    86|\n",
      "|         4|  Gaurav Acharekar|    93|\n",
      "|         5|      Shadab Azeem|    96|\n",
      "|         6|      Rachit Sapra|    86|\n",
      "|         7|     Tulika Mittal|    90|\n",
      "|         8|         Narhari B|    81|\n",
      "|         9|      Akash Saxena|    94|\n",
      "|        10|       Heba Nomani|    96|\n",
      "|        11|     Manish Shukla|    96|\n",
      "|        12|  Aishwary Mandloi|    84|\n",
      "|        13|     Praveen Nagel|    91|\n",
      "|        14|        Viral Jani|    90|\n",
      "|        15|     Charls Joseph|    81|\n",
      "|        18|        Santosh Tv|    88|\n",
      "|        19|       Maaz Ansari|    98|\n",
      "|        20|  Sangamesh Kalagi|    82|\n",
      "|        24|         Surya Das|    90|\n",
      "|        25|Venkata Chippagiri|    96|\n",
      "|        27| Mayank Srivastava|    87|\n",
      "|        28|     Mithu Goswami|    99|\n",
      "|        29|    Sandeep Challa|    83|\n",
      "+----------+------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLeftSemiJoin = dfScores.join(dfLocation, on=['Fractal_ID'], how='leftsemi')\n",
    "dfLeftSemiJoin.show(n=23)\n",
    "dfLeftSemiJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross join \n",
    "\n",
    "Cross join returns a cartesian product of the two dataframes. So if we have \"m\" rows in left dataframe and \"n\" rows in right dataframe, what we would get as a result of cross join between two dataframe is \"m x n\" rows.\n",
    "\n",
    "As we know we have 30 rows in dfScores, and 26 rows in dfLocation, we would receive (30 x 26) = 780 rows.\n",
    "\n",
    "Avoid doing this on big dataframe. Typical uses cases might be in machine learning models, where you might need a cartesian product across two sets to produce a set of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+--------------+----------+\n",
      "|Fractal_ID|          Name|Scores|       Country|      City|\n",
      "+----------+--------------+------+--------------+----------+\n",
      "|         1|Piyusha Biswas|    99| United States|Louisville|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India|    Mumbai|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|United Kingdom|    London|\n",
      "|         1|Piyusha Biswas|    99|         India| Bengaluru|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India|    Mumbai|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India| Bengaluru|\n",
      "|         1|Piyusha Biswas|    99|         India|   Gurgaon|\n",
      "|         1|Piyusha Biswas|    99|         India|    Mumbai|\n",
      "|         1|Piyusha Biswas|    99| United States|   Atlanta|\n",
      "|         1|Piyusha Biswas|    99|         India| Bengaluru|\n",
      "|         1|Piyusha Biswas|    99|         India|    Mumbai|\n",
      "|         1|Piyusha Biswas|    99|         India| Bengaluru|\n",
      "|         1|Piyusha Biswas|    99|         India|    Mumbai|\n",
      "|         1|Piyusha Biswas|    99| United States|  New York|\n",
      "+----------+--------------+------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCrossJoin = dfScores.crossJoin(dfLocation)\n",
    "\n",
    "#----- Same functionality, Restrict number of columns returned into the new dataframe\n",
    "#dfCrossJoin = dfScores.crossJoin(dfLocation).select(\"Name\", \"Scores\", \"City\")\n",
    "\n",
    "\n",
    "#-----  Produces same number of results - except here you provide the right side of the cartesian product\n",
    "#-----  and only \"Country\" and \"City\" column is returned from the dfLocation table.\n",
    "#dfCrossJoin = dfScores.crossJoin(dfLocation.select(\"Country\", \"City\"))\n",
    "\n",
    "dfCrossJoin.show()\n",
    "dfCrossJoin.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the above query, we have not specified the dataframe to which the column belongs to. What if we had specified \"Fractal_ID\" which belongs to both the dataframes? We would get an error as demonstrated in the execution of the code below - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCrossJoin = dfScores.crossJoin(dfLocation).select(\"Fractal_ID\", \"Name\", \"Scores\", \"City\")\n",
    "\n",
    "#----- To avoid such  an error\n",
    "#dfCrossJoin = dfScores.crossJoin(dfLocation).select(dfScores.Fractal_ID, \"Name\", \"Scores\", \"City\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;\"><i>Extra Gyaan: Spark would do a cartesian join, if you just specificy the join without providing the condition on join. An \"AnalysisException\" exception is thrown when the user forgets to give a condition on join. To bypass \"AnalysisException\", you can set a configuration variable - \"spark.sql.crossjoin.enabled=true\", however ... NEVER DO THAT. This is Spark way of restricting you from accidentally triggering a cartesian join, by not specifying join condition.<span style=\"color:green;\"><i>Extra Gyaan: An \"entry point\" is defined as a point where control is transferred from operating system to the provided program.</i></span>\n",
    "    \n",
    "Try executing the line below, and read the error output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfErrorJoin = dfScores.join(dfLocation)\n",
    "dfErrorJoin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is that all.. what if I have to put a join using more than one column?\n",
    "\n",
    "Yes, there would be situations where you would require to have joins on more than two columns. However remember in our syntax the order matter. One cannot have for example - \n",
    "df1.join(df2, (column_set1), (column_set2), join_type)\n",
    "\n",
    "The order matters. We need to put the conditions in square bracket, but to demonstrate that, let us load some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAttempts = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/dataAttempts.csv')\n",
    "\n",
    "# Location of the Participants - some of the participants location have been delete\n",
    "# some dummy Ids and location have been added.\n",
    "dfAttemptScores = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/dataAttemptScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+\n",
      "|Fractal_ID|               Name|Attempt|\n",
      "+----------+-------------------+-------+\n",
      "|         1|     Piyusha Biswas|      1|\n",
      "|         2|Siddhartha Nuthakki|      2|\n",
      "|         3|     Phani Kompella|      2|\n",
      "|         4|   Gaurav Acharekar|      2|\n",
      "|         5|       Shadab Azeem|      2|\n",
      "|         6|       Rachit Sapra|      2|\n",
      "|         7|      Tulika Mittal|      1|\n",
      "|         8|          Narhari B|      1|\n",
      "|         9|       Akash Saxena|      2|\n",
      "|        10|        Heba Nomani|      1|\n",
      "|        11|      Manish Shukla|      2|\n",
      "|        12|   Aishwary Mandloi|      1|\n",
      "|        13|      Praveen Nagel|      3|\n",
      "|        14|         Viral Jani|      3|\n",
      "|        15|      Charls Joseph|      2|\n",
      "|        16|     Affan Mohammed|      1|\n",
      "|        17|       Ashish Aswal|      3|\n",
      "|        18|         Santosh Tv|      2|\n",
      "|        19|        Maaz Ansari|      1|\n",
      "|        20|   Sangamesh Kalagi|      3|\n",
      "|        21|       Madhusudan B|      1|\n",
      "|        22|        Muniswamy S|      1|\n",
      "|        23|     Prashant Yadav|      1|\n",
      "|        24|          Surya Das|      1|\n",
      "|        25| Venkata Chippagiri|      3|\n",
      "|        26|       Versha Singh|      1|\n",
      "|        27|  Mayank Srivastava|      1|\n",
      "|        28|      Mithu Goswami|      1|\n",
      "|        29|     Sandeep Challa|      1|\n",
      "+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAttempts.show(n=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n",
      "|Fractal_ID|Attempt|Score|\n",
      "+----------+-------+-----+\n",
      "|         1|      1|   99|\n",
      "|         2|      1|   64|\n",
      "|         2|      2|   92|\n",
      "|         3|      1|   67|\n",
      "|         3|      2|   86|\n",
      "|         4|      1|   72|\n",
      "|         4|      2|   93|\n",
      "|         5|      1|   71|\n",
      "|         5|      2|   96|\n",
      "|         6|      1|   68|\n",
      "|         6|      2|   86|\n",
      "|         7|      1|   90|\n",
      "|         8|      1|   81|\n",
      "|         9|      1|   72|\n",
      "|         9|      2|   94|\n",
      "|        10|      1|   96|\n",
      "|        11|      1|   61|\n",
      "|        11|      2|   96|\n",
      "|        12|      1|   84|\n",
      "|        13|      1|   72|\n",
      "|        13|      2|   72|\n",
      "|        13|      3|   91|\n",
      "|        14|      1|   72|\n",
      "|        14|      2|   71|\n",
      "|        14|      3|   90|\n",
      "|        15|      1|   74|\n",
      "|        15|      2|   81|\n",
      "|        16|      1|   88|\n",
      "|        17|      1|   61|\n",
      "|        17|      2|   68|\n",
      "|        17|      3|  100|\n",
      "|        18|      1|   64|\n",
      "|        18|      2|   88|\n",
      "|        19|      1|   98|\n",
      "|        20|      1|   63|\n",
      "|        20|      2|   63|\n",
      "|        20|      3|   82|\n",
      "|        21|      1|   94|\n",
      "|        22|      1|   82|\n",
      "|        23|      1|   99|\n",
      "|        24|      1|   90|\n",
      "|        25|      1|   75|\n",
      "|        25|      2|   69|\n",
      "|        25|      3|   96|\n",
      "|        26|      1|   93|\n",
      "|        27|      1|   87|\n",
      "|        28|      1|   99|\n",
      "|        29|      1|   83|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAttemptScores.show(n=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output we want is a dataframe with all the final \"Attempt\" scores -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+-----+\n",
      "|Fractal_ID|               Name|Attempt|Score|\n",
      "+----------+-------------------+-------+-----+\n",
      "|         1|     Piyusha Biswas|      1|   99|\n",
      "|         2|Siddhartha Nuthakki|      2|   92|\n",
      "|         3|     Phani Kompella|      2|   86|\n",
      "|         4|   Gaurav Acharekar|      2|   93|\n",
      "|         5|       Shadab Azeem|      2|   96|\n",
      "|         6|       Rachit Sapra|      2|   86|\n",
      "|         7|      Tulika Mittal|      1|   90|\n",
      "|         8|          Narhari B|      1|   81|\n",
      "|         9|       Akash Saxena|      2|   94|\n",
      "|        10|        Heba Nomani|      1|   96|\n",
      "|        11|      Manish Shukla|      2|   96|\n",
      "|        12|   Aishwary Mandloi|      1|   84|\n",
      "|        13|      Praveen Nagel|      3|   91|\n",
      "|        14|         Viral Jani|      3|   90|\n",
      "|        15|      Charls Joseph|      2|   81|\n",
      "|        16|     Affan Mohammed|      1|   88|\n",
      "|        17|       Ashish Aswal|      3|  100|\n",
      "|        18|         Santosh Tv|      2|   88|\n",
      "|        19|        Maaz Ansari|      1|   98|\n",
      "|        20|   Sangamesh Kalagi|      3|   82|\n",
      "|        21|       Madhusudan B|      1|   94|\n",
      "|        22|        Muniswamy S|      1|   82|\n",
      "|        23|     Prashant Yadav|      1|   99|\n",
      "|        24|          Surya Das|      1|   90|\n",
      "|        25| Venkata Chippagiri|      3|   96|\n",
      "|        26|       Versha Singh|      1|   93|\n",
      "|        27|  Mayank Srivastava|      1|   87|\n",
      "|        28|      Mithu Goswami|      1|   99|\n",
      "|        29|     Sandeep Challa|      1|   83|\n",
      "+----------+-------------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFinalScores = dfAttempts.join(dfAttemptScores, [dfAttempts.Fractal_ID == dfAttemptScores.Fractal_ID,\n",
    "                                           dfAttempts.Attempt == dfAttemptScores.Attempt], \n",
    "                                           how='inner').select(dfAttempts.Fractal_ID, \n",
    "                                            \"Name\", dfAttempts.Attempt, \"Score\")\n",
    "\n",
    "#---- To make the above code more readable - we can rewrite our code as \n",
    "#condition = [dfAttempts.Fractal_ID == dfAttemptScores.Fractal_ID,\n",
    "#              dfAttempts.Attempt == dfAttemptScores.Attempt]\n",
    "#dfFinalScores = dfAttempts.join(dfAttemptScores, condition, \"inner\") \\\n",
    "#                .select(dfAttempts.Fractal_ID, \"Name\", dfAttempts.Attempt, \"Score\")\n",
    "\n",
    "dfFinalScores.show(n=29)\n",
    "dfFinalScores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I want to do joins on more than one dataframes?\n",
    "\n",
    "Use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+-----+--------------+----------+\n",
      "|Fractal_ID|               Name|Attempt|Score|       Country|      City|\n",
      "+----------+-------------------+-------+-----+--------------+----------+\n",
      "|         1|     Piyusha Biswas|      1|   99|         India|   Gurgaon|\n",
      "|         1|     Piyusha Biswas|      1|   99| United States|Louisville|\n",
      "|         2|Siddhartha Nuthakki|      2|   92|          null|      null|\n",
      "|         3|     Phani Kompella|      2|   86|         India|   Gurgaon|\n",
      "|         4|   Gaurav Acharekar|      2|   93|         India|    Mumbai|\n",
      "|         5|       Shadab Azeem|      2|   96|         India|   Gurgaon|\n",
      "|         6|       Rachit Sapra|      2|   86|         India|   Gurgaon|\n",
      "|         7|      Tulika Mittal|      1|   90|United Kingdom|    London|\n",
      "|         8|          Narhari B|      1|   81|         India| Bengaluru|\n",
      "|         9|       Akash Saxena|      2|   94|         India|   Gurgaon|\n",
      "|        10|        Heba Nomani|      1|   96|         India|    Mumbai|\n",
      "|        11|      Manish Shukla|      2|   96|         India|   Gurgaon|\n",
      "|        12|   Aishwary Mandloi|      1|   84|         India| Bengaluru|\n",
      "|        13|      Praveen Nagel|      3|   91|         India|   Gurgaon|\n",
      "|        14|         Viral Jani|      3|   90|         India|    Mumbai|\n",
      "|        15|      Charls Joseph|      2|   81| United States|   Atlanta|\n",
      "|        16|     Affan Mohammed|      1|   88|          null|      null|\n",
      "|        17|       Ashish Aswal|      3|  100|          null|      null|\n",
      "|        18|         Santosh Tv|      2|   88|         India| Bengaluru|\n",
      "|        19|        Maaz Ansari|      1|   98|         India|    Mumbai|\n",
      "+----------+-------------------+-------+-----+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfThreeDf = dfAttempts.join(dfAttemptScores, [\"Fractal_ID\",\"Attempt\"]) \\\n",
    "                .join(dfLocation, \"Fractal_ID\", \"left\") \\\n",
    "                .select(dfAttempts.Fractal_ID, \"Name\", dfAttempts.Attempt, \"Score\", \"Country\", \"City\")\n",
    "\n",
    "\n",
    "dfThreeDf.show()\n",
    "dfThreeDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous result, we had Piyusha Biswas being repeated twice, because there are two entries of Piyusha Biswas in the table dfLocation.\n",
    "\n",
    "There would be more complex combination with other Spark functions, we would demonstrate them in the Chapter - Bringing it all together, once you are settled with the basics. We would also discuss Shuffle operations and optimizing them in our Chapter - Optimizing Spark. \n",
    "\n",
    "To give you an idea :\n",
    "- what if you want to get the first match of Piyusha Biswas, and ignore subsequent (or the last one - hopefully sorted by say - Date, in the sense latest record) records? Hint - The approach would be to first get the output by joins, and then do the operation to get the record with say either max Date, or first match or similar. \n",
    "- What if you want to get the scores of the first attempt of all the participants? Hint - the approach would be combining it with Fractal_ID only as join, and then take the min Attempt or Attemp == 1. \n",
    "\n",
    "Let us look at an example, where we want to get the all the scores. Now that we have this dataset, we can find average score, first attempt score, or any other measure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+-----+\n",
      "|Fractal_ID|               Name|Attempt|Score|\n",
      "+----------+-------------------+-------+-----+\n",
      "|         1|     Piyusha Biswas|      1|   99|\n",
      "|         2|Siddhartha Nuthakki|      2|   92|\n",
      "|         2|Siddhartha Nuthakki|      1|   64|\n",
      "|         3|     Phani Kompella|      2|   86|\n",
      "|         3|     Phani Kompella|      1|   67|\n",
      "|         4|   Gaurav Acharekar|      2|   93|\n",
      "|         4|   Gaurav Acharekar|      1|   72|\n",
      "|         5|       Shadab Azeem|      2|   96|\n",
      "|         5|       Shadab Azeem|      1|   71|\n",
      "|         6|       Rachit Sapra|      2|   86|\n",
      "|         6|       Rachit Sapra|      1|   68|\n",
      "|         7|      Tulika Mittal|      1|   90|\n",
      "|         8|          Narhari B|      1|   81|\n",
      "|         9|       Akash Saxena|      2|   94|\n",
      "|         9|       Akash Saxena|      1|   72|\n",
      "|        10|        Heba Nomani|      1|   96|\n",
      "|        11|      Manish Shukla|      2|   96|\n",
      "|        11|      Manish Shukla|      1|   61|\n",
      "|        12|   Aishwary Mandloi|      1|   84|\n",
      "|        13|      Praveen Nagel|      3|   91|\n",
      "+----------+-------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAllScores = dfAttempts.join(dfAttemptScores, on=['Fractal_ID']) \\\n",
    "              .select(dfAttempts.Fractal_ID, dfAttempts.Name,\n",
    "                      dfAttemptScores.Attempt, dfAttemptScores.Score)\n",
    "dfAllScores.show()\n",
    "dfAllScores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method chaining is good, howevever it is recommended that in beginning you break your problem in multiple steps to ensure that what data is being returned is as per your expectations. Once you have the final output verified, you can refactor the code, and clean it up. \n",
    "\n",
    "A word on Method Chaining - method chaining might not provide you any performance gain. Spark follows a \"lazy mechanism\", and would not perform any processing on \"transform\" steps, till it faces an \"action\" step such as \".show()\". Spark look at all the previous steps, and optimizes the code for you. We would discuss more on Method chaining in our chapter - Optimizing Spark.\n",
    "\n",
    "For now, let us move to the next notebook - Aggregations and GroupBys. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
