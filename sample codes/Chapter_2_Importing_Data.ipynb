{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Recipes: Chapter 2: Importing/Reading Data\n",
    "\n",
    "There are multiple ways to read any file in Spark. We begin with the most popular format CSV and proceed to reading Text and Parquet formats. \n",
    "\n",
    "One can read the csv file in Spark using \n",
    "- spark.read.csv()\n",
    "- spark.read.format()\n",
    "\n",
    "The advisable option is to use the spark.read.format(), and used frequently in this notebook. We have included examples for the other aproaches also in the recipe.\n",
    "\n",
    "Important parameters for the API csv()-<br/> (adapted from - https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html)\n",
    "\n",
    "<i>csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, \n",
    "   inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, \n",
    "   nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, \n",
    "   maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, \n",
    "   multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)</i>\n",
    "\n",
    "- path – string, or list of strings, for input path(s).\n",
    "- schema – an optional pyspark.sql.types.StructType for the input schema.\n",
    "- header – uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "- inferSchema – infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.\n",
    "- sep (if using spark.read.csv() – sets the single character as a separator for each field and value. If None is set, it uses the default value, ,. (note: one can also use \"delimiter\")\n",
    "- encoding – decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8.\n",
    "- dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "- timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss.SSSXXX.\n",
    "- quote – sets the single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "- escape – sets the single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \\.\n",
    "- ignoreLeadingWhiteSpace – A flag indicating whether or not leading whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "- ignoreTrailingWhiteSpace – A flag indicating whether or not trailing whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "- nullValue – sets the string representation of a null value. If None is set, it uses the default value, empty string. Since 2.0.1, this nullValue param applies to all supported types including the string type.\n",
    "- nanValue – sets the string representation of a non-number value. If None is set, it uses the default value, NaN.\n",
    "\n",
    "- comment – sets the single character used for skipping lines beginning with this character (e.g. \"#\"). By default (None), it is disabled. \n",
    "- maxColumns – defines a hard limit of how many columns a record can have. If None is set, it uses the default value, 20480.\n",
    "- maxCharsPerColumn – defines the maximum number of characters allowed for any given value being read. If None is set, it uses the default value, -1 meaning unlimited length.\n",
    "- mode - this one is interesting, by default the mode is \"PERMISSIVE\", sets other fields to null when it meets a corrupted record. When a length of parsed CSV tokens is shorter than an expected length of a schema, it sets null for extra fields. \"DROPMALFORMED\" would ignore the whole corrupted records, and \"FAILFAST\" would throw an exception, the moment you meet the corrupted records. \n",
    "- multiLine – parse records, which may span multiple lines. If None is set, it uses the default value, false.\n",
    "\n",
    "nullValue and nanValue would follow default behaviour - empty string would be taken as string, and non-number value would be treated as NaN.\n",
    "\n",
    "Enough theory - now let us get back to practicals :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# creating a SparkSession object - you can change any of the configuration option you like. Remember this would\n",
    "# get the existsing SparkSession and would not create a new one.\n",
    "# So in case your previous notebook is still running - no issues\n",
    "sparkSession = SparkSession \\\n",
    "                .builder \\\n",
    "                .master(\"local\") \\\n",
    "                .appName(\"Pyspark Recipes - Importing Data\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "# Remember in the previous session we had imported a CSV file\n",
    "dfCensus = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .load('../datasets/charityml/censusdata.csv')\n",
    "\n",
    "print(dfCensus.count())\n",
    "print(time.time()-startTime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now to get the Schema of Datatypes (dtypes) of the dataframe below are the two methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This would give us the Schema of the text file\n",
    "dfCensus.printSchema()\n",
    "\n",
    "#One can also get just the datatypes by using - \n",
    "dfCensus.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;\"><i>Extra Gyaan</i></span>\n",
    "\n",
    "At least till now, there is not an elegant way to know a datatype of a single column. If there is a situtation \n",
    "where you have to check the datatype of a single column, this is the trick we adopt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(dfCensus.dtypes)['education-num']\n",
    "\n",
    "## OR ##\n",
    "# uncomment the following 3 lines\n",
    "\n",
    "#def getDataType(df,columnName):\n",
    "#    return [dtype for name, dtype in df.dtypes if name == columnName][0]\n",
    "\n",
    "#getDataType(dfCensus,'income')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;\"><i>Extra Gyaan</i></span>\n",
    "\n",
    "This would be typical way to read a CSV file. However, while reading a text file, without defining a schema would require two pass through the entire file, so that the schema can be applied across. Spark unfortunately does not infer the schema using a sample. This could lead to time taking more time than usual than reading a large say 10 GB file. \n",
    "\n",
    "We have a nifty small code which help us infer the schema, which we are sharing with you. The steps you need ot take are \n",
    "- You can take a sample 10,100,1000 rows, instead of taking the entire large file,\n",
    "- Check the schema, make changes if necessary\n",
    "- Reimport the file with the defined schema\n",
    "\n",
    "** Remember this is complete optional, but could save time **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we get a list of 10 rows from the CSV file\n",
    "\n",
    "startTime = time.time()\n",
    "lstSample = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, delimiter = \",\") \\\n",
    "            .load('../datasets/charityml/censusdata.csv').head(10)\n",
    "\n",
    "print(time.time()-startTime)\n",
    "\n",
    "lstSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as pst#pst stands for (p)yspark(s)ql(t)ypes\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# a function to help infer schema\n",
    "def infer_schema(rec):\n",
    "    \"\"\"infers dataframe schema for a record. Assumes every dict is a Struct, not a Map\"\"\"\n",
    "    if isinstance(rec, dict):\n",
    "        return pst.StructType([pst.StructField(key, infer_schema(value), True)\n",
    "                              for key, value in sorted(rec.items())])\n",
    "    elif isinstance(rec, list):\n",
    "        if len(rec) == 0:\n",
    "            raise ValueError(\"can't infer type of an empty list\")\n",
    "        elem_type = infer_schema(rec[0])\n",
    "        for elem in rec:\n",
    "            this_type = infer_schema(elem)\n",
    "            if elem_type != this_type:\n",
    "                raise ValueError(\"can't infer type of a list with inconsistent elem types\")\n",
    "        #return pst.ArrayType(elem_type)\n",
    "        return elem_type\n",
    "    else:\n",
    "        return pst._infer_type(rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = infer_schema(lstSample)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Remember in the previous session we had imported a CSV file\n",
    "dfCensus = sparkSession.read.format('csv') \\\n",
    "            .options(header = True, inferSchema = True, sep = \",\", enforceSchema = True,\n",
    "                ignoreLeadingWhiteSpace = True, ignoreTrailingWhiteSpace = True) \\\n",
    "            .schema(schema) \\\n",
    "            .load('../datasets/charityml/censusdata.csv')\n",
    "\n",
    "print(dfCensus.count())\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the import time while reading a text file, with schema defined, versus schema not defined. Feel free to use any of the important options that are available with the .csv() method\n",
    "\n",
    "Another tip to improve your operation is to use parquet file. Any intermediate files that needs to be written back to disk, should ideally be written in parquet format. There are two main benefits -\n",
    "- The file stored is very compressed, for example, the near about 5-6 MB file we are reading would take around 1-1.5 MB space, if written back in parquet format\n",
    "- The file is stored with schema, one does not have to worry about infering schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCensus.write.parquet('census.parquet', mode='overwrite')\n",
    "\n",
    "startTime = time.time()\n",
    "dfCensus = sparkSession.read.parquet('census.parquet')\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCensus.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would continue to work on this dataframe, in our next notebook we would look at EDAs (Exploratory Data Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
